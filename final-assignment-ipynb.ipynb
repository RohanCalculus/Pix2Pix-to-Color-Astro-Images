{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1544285,"sourceType":"datasetVersion","datasetId":910930}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Your tasks:- \n- You need to comment the code per code cell as detailed as possible\n- You need to improve the PSNR value to more than 20 (in the given notebook we get about 17-18 PSNR)\n- You need to document everything in a word file and submit a PDF version of the changes and corresponding PSNR scores","metadata":{}},{"cell_type":"markdown","source":"# **Importing libraries**","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf \nimport os \nimport pathlib\nfrom pathlib import Path\nimport time \nimport datetime \n\nfrom matplotlib import pyplot as plt \nfrom IPython.display import display\nfrom IPython.display import clear_output ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Importing dataset**","metadata":{}},{"cell_type":"code","source":"dataset_name = 'Hubble telescope images'\ndataset_pathname = \"/kaggle/input/top-100-hubble-telescope-images\"\n# Convert the string path to a Path object\ndataset_path = Path(dataset_pathname)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Checking the dataset","metadata":{}},{"cell_type":"code","source":"number_of_images=len(list(dataset_path.iterdir()))\nprint(f\"Number of images:{number_of_images}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nfor image_path in dataset_path.iterdir():\n        with Image.open(image_path) as img:\n            plt.imshow(img)\n            plt.axis('off') \n            plt.show()\n        break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Resizing and converting images from .tif to .jpeg","metadata":{}},{"cell_type":"markdown","source":"Tensorflow does not work well with .tif images hence we call a function to convert the images and resize them to prevent memory issues and maintain consistency","metadata":{}},{"cell_type":"code","source":"import cv2\ndataset_path = Path(\"/kaggle/input/top-100-hubble-telescope-images\")  \noutput_path = Path(\"/kaggle/working/\")  \n\ntarget_height = 256\ntarget_width = 256\n\ndef resize_image(image, target_height, target_width):\n    resized_image = cv2.resize(image, (target_width, target_height), interpolation=cv2.INTER_CUBIC)\n    return resized_image\n\nfor file in dataset_path.iterdir():\n    if file.is_file():\n        file_path = str(file)\n        \n        image = cv2.imread(file_path)\n\n        resized_image = resize_image(image, target_height, target_width)\n        \n        # Changing extension to JPEG\n        output_file_path = output_path / (file.stem + '.jpg')\n        \n        cv2.imwrite(str(output_file_path), resized_image, [int(cv2.IMWRITE_JPEG_QUALITY), 90])\n        \n        print(f\"Converted and resized {file.name} to {output_file_path.name}\")\n\nprint(\"Conversion and resizing complete.\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Calling a function to store them real images separately and convert them into tensors.**","metadata":{}},{"cell_type":"code","source":"def ogimages(datapath):\n    ogimages = []\n    file_paths = []\n    \n    for file in datapath.rglob('*'):\n        if file.is_file() and file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.tif', '.tiff']:\n            file_path = str(file)\n            \n            image = tf.io.read_file(file_path)\n            image = tf.io.decode_image(image)\n            \n            image = tf.cast(image, tf.float32)\n            \n            ogimages.append(image)\n            \n            file_paths.append(file_path)\n    \n    return ogimages, file_paths\n\nreal_images, rfiles = ogimages(output_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Type of rfiles: {type(rfiles)}\")\nprint(f\"Contents of rfiles: {rfiles[:5]}\")  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Converting to b/w\nCalling a function to convert input images into black and white images and store them as tensors.","metadata":{}},{"cell_type":"code","source":"def input_images1(datapath):\n    input_images = []\n    file_paths = []\n    \n    for file in datapath.rglob('*'):\n        if file.is_file() and file.suffix.lower() in ['.jpg', '.jpeg', '.png', '.tif', '.tiff']:\n            file_path = str(file) \n            \n            image = tf.io.read_file(file_path)\n            image = tf.io.decode_image(image) \n           \n            image = tf.cast(image, tf.float32)\n            \n            if image.shape[-1] == 3:  # Check if the image has 3 channels (RGB)\n                image = tf.image.rgb_to_grayscale(image)\n            \n            input_images.append(image)\n            \n            # Also append the file path to the file paths list\n            file_paths.append(file_path)            \n    \n    # Return the list of grayscale images as float32 tensors\n    return input_images, file_paths\n\nbnw_images, bnwfiles = input_images1(output_path)\n\nprint(f\"Number of grayscale images loaded: {len(bnw_images)}\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking if functions have been executed properly","metadata":{}},{"cell_type":"code","source":"print(f\"Real image shape: {real_images[0].shape}, dtype: {real_images[0].dtype}\")\nprint(f\"Grayscale image shape: {bnw_images[0].shape}, dtype: {bnw_images[0].dtype}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nreal_image= real_images[1]\ninput_image = bnw_images[1] \n\nplt.figure(figsize=(12, 6))\n\nplt.subplot(1, 2, 1)\nplt.imshow(real_image/255.0)\nplt.title('Real Image')\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nplt.imshow(input_image/255.0, cmap='gray') \nplt.title('Input Image (B&w)')\nplt.axis('off')\n\n# Show the plot\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Splitting the dataset into training, testing and validation sets","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport shutil\n\n# Define the split ratios\ntest_ratio = 0.2 \nval_ratio = 0.1   \n\ndata_pairs = list(zip(real_images, bnw_images))\n\ntrain_data, temp_data = train_test_split(data_pairs, test_size=test_ratio, random_state=42)\nval_data, test_data = train_test_split(temp_data, test_size=val_ratio / (1 - test_ratio), random_state=42)\n\ntrain_path = output_path / \"train\"\nval_path = output_path / \"val\"\ntest_path = output_path / \"test\"\n\ntrain_path.mkdir(parents=True, exist_ok=True)\nval_path.mkdir(parents=True, exist_ok=True)\ntest_path.mkdir(parents=True, exist_ok=True)\n\ndef save_images(data_pairs, directory, real_subdir, input_subdir):\n    real_dir = directory / real_subdir\n    input_dir = directory / input_subdir\n    real_dir.mkdir(parents=True, exist_ok=True)\n    input_dir.mkdir(parents=True, exist_ok=True)\n    \n    for idx, (real_img, bnw_img) in enumerate(data_pairs):\n        # Define file paths for the new images\n        real_path = real_dir / f\"image_{idx}.jpg\"\n        bnw_path = input_dir / f\"image_{idx}.jpg\"\n        \n        tf.keras.utils.save_img(real_path, real_img, scale=True)\n        \n        tf.keras.utils.save_img(bnw_path, bnw_img, scale=True)\n\nsave_images(train_data, train_path, \"real\", \"input\")\nsave_images(val_data, val_path, \"real\", \"input\")\nsave_images(test_data, test_path, \"real\", \"input\")\n\nprint(\"Data split and saved into training, validation, and testing sets.\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepocessing images","metadata":{}},{"cell_type":"code","source":"BUFFER_SIZE = 99\nBATCH_SIZE = 1\nIMG_WIDTH = 256\nIMG_HEIGHT = 256","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def resize(input_image, real_image, height, width):\n  input_image = tf.image.resize_with_pad(input_image,target_height=height, target_width=width)                             \n  real_image = tf.image.resize_with_pad(real_image,target_height=height, target_width=width)                             \n\n  return input_image, real_image\n\ndef random_crop(input_image, real_image):\n    min_channels = min(input_image.shape[-1], real_image.shape[-1])\n    cropped_input = tf.image.random_crop(input_image, size=[IMG_HEIGHT, IMG_WIDTH, min_channels])\n    cropped_real = tf.image.random_crop(real_image, size=[IMG_HEIGHT, IMG_WIDTH, 3])\n\n    return cropped_input, cropped_real\n\n@tf.function()\ndef random_jitter(input_image, real_image):\n    input_image, real_image = resize(input_image, real_image, 286, 286)\n    input_image, real_image = random_crop(input_image, real_image)\n\n    if tf.random.uniform([]) > 0.5:\n        input_image = tf.image.flip_left_right(input_image)\n        real_image = tf.image.flip_left_right(real_image)\n\n    return input_image, real_image\n\n\ndef normalize(input_image, real_image):\n    input_image = tf.cast(input_image, tf.float32)\n    real_image = tf.cast(real_image, tf.float32)\n    \n    input_image = (input_image / 127.5) - 1\n    real_image = (real_image / 127.5) - 1\n    \n    return input_image, real_image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reviewing preprocessing functions","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport random\n\nidx = random.randint(0, len(real_images) - 1)\ninp = bnw_images[idx]  \nre = real_images[idx]  \n\nplt.figure(figsize=(6, 6))\nfor i in range(4):\n    rj_inp, rj_re = random_jitter(inp, re)\n    \n    plt.subplot(2, 2, i + 1)\n    plt.imshow(rj_inp / 255.0,cmap='gray')\n    plt.axis('off')\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating an input data pipeline","metadata":{}},{"cell_type":"code","source":"def load_image_pair(input_path, real_path):\n    print(f\"load_image_pair - input_path type: {type(input_path)}, real_path type: {type(real_path)}\")\n    print(f\"load_image_pair - input_path value: {input_path}, real_path value: {real_path}\")\n\n    input_image = tf.io.read_file(input_path)\n    real_image = tf.io.read_file(real_path)\n\n    print(f\"load_image_pair - input_image type: {type(input_image)}, real_image type: {type(real_image)}\")\n\n    input_image = tf.io.decode_jpeg(input_image, channels=1)  # Load grayscale image\n    real_image = tf.io.decode_jpeg(real_image, channels=3)  # Load RGB image\n\n    print(f\"load_image_pair - final input_image type: {type(input_image)}, final real_image type: {type(real_image)}\")\n\n    return input_image, real_image\n\ndef preprocess_image_pair(input_path, real_path):\n    print(f\"preprocess_image_pair - input_path type: {type(input_path)}, real_path type: {type(real_path)}\")\n\n    input_image, real_image = load_image_pair(input_path, real_path)\n\n    print(f\"preprocess_image_pair - input_image type: {type(input_image)}, real_image type: {type(real_image)}\")\n\n    input_image, real_image = resize(input_image, real_image, 286, 286)\n    input_image, real_image = random_crop(input_image, real_image)\n    input_image, real_image = normalize(input_image, real_image)\n\n    print(f\"preprocess_image_pair - final input_image type: {type(input_image)}, final real_image type: {type(real_image)}\")\n\n    return input_image, real_image\n\ndef create_dataset(input_paths, real_paths, batch_size, shuffle=True):\n    dataset = tf.data.Dataset.from_tensor_slices((input_paths, real_paths))\n    \n    print(f\"create_dataset - dataset: {dataset}\")\n    print(f\"create_dataset - input_paths type: {type(input_paths)}, real_paths type: {type(real_paths)}\")\n\n    dataset = dataset.map(preprocess_image_pair, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\n    if shuffle:\n        dataset = dataset.shuffle(buffer_size=len(input_paths))\n\n    dataset = dataset.batch(batch_size)\n\n    return dataset\n\nbatch_size = 1\ntrain_dataset = create_dataset(rfiles, bnwfiles, batch_size=batch_size)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess_test_image(input_path, real_path):\n    print(f\"preprocess_test_image - input_path type: {type(input_path)}, real_path type: {type(real_path)}\")\n    print(f\"preprocess_test_image - input_path value: {input_path}, real_path value: {real_path}\")\n\n    input_image = tf.io.read_file(input_path)\n    real_image = tf.io.read_file(real_path)\n\n    print(f\"preprocess_test_image - input_image type: {type(input_image)}, real_image type: {type(real_image)}\")\n\n    input_image = tf.io.decode_jpeg(input_image, channels=1)  # Load grayscale image\n    real_image = tf.io.decode_jpeg(real_image, channels=3)  # Load RGB image\n\n    print(f\"preprocess_test_image - decoded input_image type: {type(input_image)}, decoded real_image type: {type(real_image)}\")\n\n    input_image, real_image = resize(input_image, real_image, IMG_HEIGHT, IMG_WIDTH)\n    \n    input_image, real_image = normalize(input_image, real_image)\n    \n    print(f\"preprocess_test_image - final input_image type: {type(input_image)}, final real_image type: {type(real_image)}\")\n\n    return input_image, real_image\ndef create_test_dataset(input_paths, real_paths, batch_size=1):\n    dataset = tf.data.Dataset.from_tensor_slices((input_paths, real_paths))\n    \n    print(f\"create_test_dataset - dataset: {dataset}\")\n    print(f\"create_test_dataset - input_paths type: {type(input_paths)}, real_paths type: {type(real_paths)}\")\n\n    dataset = dataset.map(preprocess_test_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    \n    print(f\"create_test_dataset - dataset after map: {dataset}\")\n\n    dataset = dataset.batch(batch_size)\n\n    print(f\"create_test_dataset - final dataset: {dataset}\")\n\n    return dataset\n\ntest_dataset = create_test_dataset(bnwfiles, rfiles, batch_size=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining generator (U-NET ARCHITECTURE)","metadata":{}},{"cell_type":"code","source":"OUTPUT_CHANNELS = 3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Encoder","metadata":{}},{"cell_type":"code","source":"def downsample(filters, size, apply_batchnorm=True):\n  initializer = tf.random_normal_initializer(0., 0.02)\n\n  result = tf.keras.Sequential()\n  result.add(\n      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n                             kernel_initializer=initializer, use_bias=False))\n\n  if apply_batchnorm:\n    result.add(tf.keras.layers.BatchNormalization())\n\n  result.add(tf.keras.layers.LeakyReLU())\n\n  return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"down_model = downsample(3, 4)\ndown_result = down_model(tf.expand_dims(inp, 0))\nprint (down_result.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Decoder","metadata":{}},{"cell_type":"code","source":"def upsample(filters, size, apply_dropout=False):\n  initializer = tf.random_normal_initializer(0., 0.02)\n\n  result = tf.keras.Sequential()\n  result.add(\n    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n                                    padding='same',\n                                    kernel_initializer=initializer,\n                                    use_bias=False))\n\n  result.add(tf.keras.layers.BatchNormalization())\n\n  if apply_dropout:\n      result.add(tf.keras.layers.Dropout(0.5))\n\n  result.add(tf.keras.layers.ReLU())\n\n  return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"up_model = upsample(3, 4)\nup_result = up_model(down_result)\nprint (up_result.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Combining upsampler and downsampler","metadata":{}},{"cell_type":"markdown","source":"Visualizing generator","metadata":{}},{"cell_type":"code","source":"def Generator():\n  inputs = tf.keras.layers.Input(shape=[256, 256, 1])\n\n  down_stack = [\n    downsample(64, 4, apply_batchnorm=False),  # (batch_size, 128, 128, 64)\n    downsample(128, 4),  # (batch_size, 64, 64, 128)\n    downsample(256, 4),  # (batch_size, 32, 32, 256)\n    downsample(512, 4),  # (batch_size, 16, 16, 512)\n    downsample(512, 4),  # (batch_size, 8, 8, 512)\n    downsample(512, 4),  # (batch_size, 4, 4, 512)\n    downsample(512, 4),  # (batch_size, 2, 2, 512)\n    downsample(512, 4),  # (batch_size, 1, 1, 512)\n  ]\n\n  up_stack = [\n    upsample(512, 4, apply_dropout=True),  # (batch_size, 2, 2, 1024)\n    upsample(512, 4, apply_dropout=True),  # (batch_size, 4, 4, 1024)\n    upsample(512, 4, apply_dropout=True),  # (batch_size, 8, 8, 1024)\n    upsample(512, 4),  # (batch_size, 16, 16, 1024)\n    upsample(256, 4),  # (batch_size, 32, 32, 512)\n    upsample(128, 4),  # (batch_size, 64, 64, 256)\n    upsample(64, 4),  # (batch_size, 128, 128, 128)\n  ]\n\n  initializer = tf.random_normal_initializer(0., 0.02)\n  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n                                         strides=2,\n                                         padding='same',\n                                         kernel_initializer=initializer,\n                                         activation='tanh')  # (batch_size, 256, 256, 3)\n\n  x = inputs\n\n  # Downsampling through the model\n  skips = []\n  for down in down_stack:\n    x = down(x)\n    skips.append(x)\n\n  skips = reversed(skips[:-1])\n\n  # Upsampling and establishing the skip connections\n  for up, skip in zip(up_stack, skips):\n    x = up(x)\n    x = tf.keras.layers.Concatenate()([x, skip])\n\n  x = last(x)\n\n  return tf.keras.Model(inputs=inputs, outputs=x)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generator = Generator()\ntf.keras.utils.plot_model(generator, show_shapes=True, dpi=64)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing the generator","metadata":{}},{"cell_type":"code","source":"gen_output = generator(inp[tf.newaxis, ...], training=False)\nplt.imshow(gen_output[0, ...])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining generator loss","metadata":{}},{"cell_type":"code","source":"LAMBDA = 100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generator_loss(disc_generated_output, gen_output, target):\n  gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n\n  # Mean absolute error\n  l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n\n  total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n\n  return total_gen_loss, gan_loss, l1_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining discriminator (PatchGan)","metadata":{}},{"cell_type":"code","source":"def Discriminator():\n  initializer = tf.random_normal_initializer(0., 0.02)\n#inp expects 1 channel while tar expects 3\n  inp = tf.keras.layers.Input(shape=[256, 256, 1], name='input_image')\n  tar = tf.keras.layers.Input(shape=[256, 256, 3], name='target_image')\n\n  x = tf.keras.layers.concatenate([inp, tar])  # (batch_size, 256, 256, channels*2)\n\n  down1 = downsample(64, 4, False)(x)  # (batch_size, 128, 128, 64)\n  down2 = downsample(128, 4)(down1)  # (batch_size, 64, 64, 128)\n  down3 = downsample(256, 4)(down2)  # (batch_size, 32, 32, 256)\n\n  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3)  # (batch_size, 34, 34, 256)\n  conv = tf.keras.layers.Conv2D(512, 4, strides=1,\n                                kernel_initializer=initializer,\n                                use_bias=False)(zero_pad1)  # (batch_size, 31, 31, 512)\n\n  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n\n  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n\n  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)  # (batch_size, 33, 33, 512)\n\n  last = tf.keras.layers.Conv2D(1, 4, strides=1,\n                                kernel_initializer=initializer)(zero_pad2)  # (batch_size, 30, 30, 1)\n\n  return tf.keras.Model(inputs=[inp, tar], outputs=last)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Visualizing discriminator","metadata":{}},{"cell_type":"code","source":"discriminator = Discriminator()\ntf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Testing discriminator","metadata":{}},{"cell_type":"code","source":"disc_out = discriminator([inp[tf.newaxis, ...], gen_output], training=False)\nplt.imshow(disc_out[0, ..., -1], vmin=-20, vmax=20, cmap='RdBu_r')\nplt.colorbar()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Defining discriminator loss","metadata":{}},{"cell_type":"code","source":"def discriminator_loss(disc_real_output, disc_generated_output):\n  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n\n  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n\n  total_disc_loss = real_loss + generated_loss\n\n  return total_disc_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Defining optimizers and checkpoint saver","metadata":{}},{"cell_type":"code","source":"generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_dir = './training_checkpoints'\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n                                 discriminator_optimizer=discriminator_optimizer,\n                                 generator=generator,\n                                 discriminator=discriminator)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generating images","metadata":{}},{"cell_type":"code","source":"def generate_images(model, test_input, tar):\n    prediction = model(test_input, training=True)\n    plt.figure(figsize=(15, 15))\n    \n    display_list = [test_input[0], tar[0], prediction[0]]\n    title = ['Input Image', 'Ground Truth', 'Predicted Image']\n    \n    for i in range(3):\n        plt.subplot(1, 3, i+1)\n        plt.title(title[i])\n        # For grayscale image, cmap='gray' is used to display it as grayscale.\n        if i == 0:\n            plt.imshow(display_list[i], cmap='gray')\n        else:\n            # Getting the pixel values in the [0, 1] range to plot.\n            plt.imshow(display_list[i] * 0.5 + 0.5)\n        plt.axis('off')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for example_input, example_target in test_dataset.take(1):\n  generate_images(generator, example_input, example_target)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note: Matplotlib automatically colors a b&w image in bgr. To avoid this, we use cmap='gray'","metadata":{}},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"log_dir=\"logs/\"\n\nsummary_writer = tf.summary.create_file_writer(\n  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calling a function to define PSNR value","metadata":{}},{"cell_type":"markdown","source":"PSNR value is the ratio of the maximum value of the pixel to the noise (MSE) that affects the quality of the pixels. It is given by:\n\nPSNR = 20 * log10(MAX) - 10 * log10(MSE)","metadata":{}},{"cell_type":"code","source":"# Define the PSNR function\ndef psnr(y_true, y_pred):\n    # Assume pixel values are in the range [0, 1]\n    max_pixel = 1.0\n    # Calculate MSE\n    mse = tf.reduce_mean(tf.square(y_true - y_pred))\n    # Calculate PSNR using the formula: PSNR = 20 * log10(MAX) - 10 * log10(MSE)\n    psnr = tf.image.psnr(y_true, y_pred, max_val=max_pixel)\n    return psnr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef train_step(input_image, target, step):\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        gen_output = generator(input_image, training=True)\n\n        disc_real_output = discriminator([input_image, target], training=True)\n        disc_generated_output = discriminator([input_image, gen_output], training=True)\n\n        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n\n    generator_gradients = gen_tape.gradient(gen_total_loss, generator.trainable_variables)\n    discriminator_gradients = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(generator_gradients, generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator.trainable_variables))\n\n    with summary_writer.as_default():\n        tf.summary.scalar('gen_total_loss', gen_total_loss, step=step//1000)\n        tf.summary.scalar('gen_gan_loss', gen_gan_loss, step=step//1000)\n        tf.summary.scalar('gen_l1_loss', gen_l1_loss, step=step//1000)\n        tf.summary.scalar('disc_loss', disc_loss, step=step//1000)\n\n    # Normalize the generator output and target to [0, 1]\n    gen_output_normalized = (gen_output + 1) / 2  \n    target_normalized = (target + 1) / 2  \n\n    # Calculate PSNR\n    psnr_value = psnr(target_normalized, gen_output_normalized)\n    return gen_total_loss, gen_gan_loss, gen_l1_loss, disc_loss, psnr_value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The actual training loop:\n\n-Iterates over the number of steps.\n\n-Every 10 steps print a dot (.).\n\n-Every 1k steps: clear the display and run generate_images to show the progress.\n\n-Every 5k steps: save a checkpoint.\n\n-Saving the generator training every 5k steps","metadata":{}},{"cell_type":"code","source":"def fit(train_ds, test_ds, steps):\n    example_input, example_target = next(iter(test_ds.take(1)))\n    start = time.time()\n\n    total_psnr = tf.constant(0.0)\n    psnr_count = tf.constant(0.0)\n\n    for step, (input_image, target) in train_ds.repeat().take(steps).enumerate():\n        if (step) % 1000 == 0:\n            clear_output(wait=True)\n\n            if step != 0:\n                print(f'Time taken for 1000 steps: {time.time()-start:.2f} sec\\n')\n\n            start = time.time()\n\n            generate_images(generator, example_input, example_target)\n            print(f\"Step: {step//1000}k\")\n\n        gen_total_loss, gen_gan_loss, gen_l1_loss, disc_loss, psnr_value = train_step(input_image, target, step)\n\n        total_psnr += tf.reduce_sum(psnr_value)\n        psnr_count += tf.cast(tf.size(psnr_value), tf.float32)\n\n        # Training step\n        if (step + 1) % 10 == 0:\n            print('.', end='', flush=True)\n\n        # Save (checkpoint) the model every 5k steps\n        if (step + 1) % 5000 == 0:\n            checkpoint.save(file_prefix=checkpoint_prefix)\n            generator.save(f'/kaggle/working/saved_generator_model_step_{step + 1}.h5')\n\n    average_psnr = total_psnr / psnr_count if psnr_count > 0 else tf.constant(0.0)\n    print(f\"\\nAverage PSNR over {psnr_count} steps: {average_psnr.numpy():.2f}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fit(train_dataset, test_dataset, steps=40000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Restore the latest checkpoint and test the network","metadata":{}},{"cell_type":"code","source":"ls {checkpoint_dir}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Restoring the latest checkpoint in checkpoint_dir\ncheckpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Run the trained model on a few examples from the test set\nfor inp, tar in test_dataset.take(5):\n    generate_images(generator, inp, tar)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **To Save the trained model**","metadata":{}},{"cell_type":"code","source":"# Save the model as HDF5 format\n#tf.keras.models.save_model(generator, '/kaggle/working/saved_model_40k.h5', save_format='h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train for additional steps","metadata":{}},{"cell_type":"code","source":"# Load the previously saved generator model\ngenerator2 = tf.keras.models.load_model('/kaggle/working/saved_generator_model_step_40000.h5')\n\n# Initialize the checkpoint manager\ncheckpoint_prefix = \"/kaggle/working/checkpoint\"\ncheckpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n                                 discriminator_optimizer=discriminator_optimizer,\n                                 generator=generator,\n                                 discriminator=discriminator)\n\n# Restore the latest checkpoint (if it includes optimizer state)\ncheckpoint.restore(tf.train.latest_checkpoint('/kaggle/working'))\n\ndef fit(train_ds, test_ds, steps):\n    example_input, example_target = next(iter(test_ds.take(1)))\n    start = time.time()\n\n    total_psnr = tf.constant(0.0)\n    psnr_count = tf.constant(0.0)\n\n    for step, (input_image, target) in train_ds.repeat().take(steps).enumerate():\n        if (step) % 1000 == 0:\n            clear_output(wait=True)\n\n            if step != 0:\n                print(f'Time taken for 1000 steps: {time.time()-start:.2f} sec\\n')\n\n            start = time.time()\n\n            generate_images(generator, example_input, example_target)\n            print(f\"Step: {step//1000}k\")\n\n        gen_total_loss, gen_gan_loss, gen_l1_loss, disc_loss, psnr_value = train_step(input_image, target, step)\n\n        total_psnr += tf.reduce_sum(psnr_value)\n        psnr_count += tf.cast(tf.size(psnr_value), tf.float32)\n\n        # Training step\n        if (step + 1) % 10 == 0:\n            print('.', end='', flush=True)\n\n        # Save (checkpoint) the model every 5k steps\n        if (step + 1) % 5000 == 0:\n            checkpoint.save(file_prefix=checkpoint_prefix)\n            generator.save(f'/kaggle/working/saved_generator_model_step_{step + 1}.h5')\n\n    average_psnr = total_psnr / psnr_count if psnr_count > 0 else tf.constant(0.0)\n    print(f\"\\nAverage PSNR over {psnr_count} steps: {average_psnr.numpy():.2f}\")\n\n# Continue training for an additional 40,000 steps\nfit(train_dataset, test_dataset, steps=40000)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualise the new generator predictions","metadata":{}},{"cell_type":"code","source":"generator3 = \"/kaggle/working/saved_generator_model_step_80000.h5\"\nfor inp, tar in test_dataset.take(5):\n    generate_images(generator, inp, tar)","metadata":{},"execution_count":null,"outputs":[]}]}